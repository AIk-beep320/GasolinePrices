---
title: "FoRECASTING GASOLINE PRICES IN OHIO"
author: "Ikya"
output:
  html_document: default
---



This project focusses on time series modeling approaches to predict the gasoline prices in Ohio for a short period of time. Modeling has been performed on the monthly data from the year 2004 to 2016 and forecast is obtained for the year 2017 i.e., 12 months, which is then compared with the actual values.  

```{r}
setwd("C:/Users/abhinav/Desktop/Ikya/courses/TimeSeries/Project")
data_pr <- read.csv("Ohio_Regular_All_Formulations_Retail_Gasoline_Prices (2) - Copy.csv", header = TRUE)
```

# Step-1: Data Cleaning 
```{r}
head(data_pr)
summary(data_pr)
```

### Arranging the data in ascending order for the purpose of analysis
```{r}
data_pr$Month <- rev(data_pr$Month)
data_pr$Price <- rev(data_pr$Price)
```

```{r}
str(data_pr)
```

# Step-2: Analyzing time series patterns and trends

```{r}
if (!require(xts)) install.packages('xts')
library(xts)
data_ts <- ts(data_pr$Price, frequency = 12, start = 2004)
plot(as.xts(data_ts), major.format = "%Y-%m",yaxis.right = FALSE, ylab = "Price in $/Gallon")
```
Prices are generally found to be lower in winters which slowly increased in spring and peaks mostly occurred in summers, when driven most frequently. Outliers were found around mid of year 2008, where the price increased drastically to $4 and reduced by the end of that year. It again increased in 2011 and dropped by the end of 2014. It seems that these fluctuations are common in this sector. And, the data is not stationary where mean and variance are not constant over time.  

### Training set (leaving the latest 12 months/1 year i.e., the year 2017)
```{r}
data_train <- data_pr$Price[1:156]
```


We have chosen Auto-Regressive Integrated Moving Average modeling for this analysis. This is a general modeling approach for predicting the time series. This forecasting generally consists of a regression equation comprising of predictors as lags of the dependent variable (Auto-Regressive-AR terms) and lags of the forecast errors (Moving Average-MA terms). The general ARIMA model is represented by ARIMA(p,d,q), where ‘p’ is the number of AR terms, ‘d’ is the number of differences required for stationarity and ‘q’ is the number of MA terms.  

Assumption of ARIMA modeling:  
- Data is stationary (In a stationary process, mean, variance and autocorrelation do not change over time)  

### Stationarity test

```{r}
if (!require(aTSA)) install.packages('aTSA')
library(aTSA)
stationary.test(data_train)
```
Augment-Dickey Fuller test:  
Hypothesis:  
Ho: Series is not stationary  
Ha: Series is stationary  
Conclusion: With p-values > 0.05, we fail to reject null hypothesis. Therefore, we do not have sufficient evidence to conclude that the time series is stationary.
To make it stationary, we performed differencing in the next step  

# Step 3: Transforming the data to make it suitable to perform time series analysis  

lag-1 differencing initially to make it stationary resulting in the below plot.  

```{r}
#lag1_data <- lag(data_train,1)
#y = data_train - lag1_data
y = diff(data_train, lag = 1)
plot(ts(y))
```

```{r}
library(aTSA)
stationary.test(diff(data_train, lag = 1))
```
From the above plot we can see that the series seems stationary with constant mean and variance over time. We confirmed this by Dickey-Fuller test.  
Augment-Dickey Fuller test:  
Hypothesis:  
Ho: Series is not stationary  
Ha: Series is stationary  
Conclusion: With p-values < 0.05, we reject null hypothesis. Therefore, we have sufficient evidence to conclude that the time series is stationary.  
This makes d = 1 in ARIMA(p,d,q) model.  

After obtaining stationarity, we might have to look for the AR and MA terms for the equation, as given in the next step.  

# Step 4: Setting up candidate models  
Three candidate models by looking at the ACF and PACF plots of the lag-1 differenced data as given below.  

```{r}
acf(diff(data_train,lag=1))
pacf(diff(data_train,lag=1))
```

Looking at the ACF plot, we can see that the high correlation is observed at lag-1 followed by a decreasing wave with alternative positive and negative correlations. This indicates a higher order autoregressive term in the model. The number of autoregressive terms can be looked from the PACF plot. At-most 7th lag has the highest significant correlation.Since, higher AR terms are not encouraged which might overfit the data, we can take AR-5 in the next candidate model.
So,
$$ Model1: ARIMA(5,1,0)$$
   
If we can perform regression on the above 5 lags to find the significant ones, the next candidate model can be, 
$$ Model2: ARIMA(5,1,0)*$$  

If we look at ACF plot again for lag-1 difference data, we can see that the 12th lag is also significant and other lags have a wavy pattern. Hence, a seasonal model can be suspected here. Even, auto.arima suggested a seasonal model.   

```{r}
library(forecast)
auto.arima(ts(data_train, frequency = 12))
```
Hence, a multiplicative seasonal model $ARIMA(5,1,0)(0,0,1)_{12}$ is chosen as a candidate model. Apart from the auto-arima’s non-seasonal part of ARIMA(0,1,1) we have taken ARIMA(5,1,0) to accommodate AR terms instead of MA terms in connection with the previous candidate models. Moreover, if MA non seasonal terms are included, it would just give a straight line as a prediction. Now, on observing further, as we see lag 1 is significant in every season, we decided an MA(1) seasonal component.  

$$ Model3: ARIMA(5,1,0)(0,0,1)_{12} $$  

# Step 5: Building models and residuals diagnosis  

## Model 1: ARIMA(5,1,0)  

```{r}
model1 = arima(ts(data_train), order = c(5,1,0))
model1
```

**Residuals Diagnosis:**  

Residuals are useful in checking whether a model has captured the adequate information in the data. We have to check if,  
o The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.  
o The residuals have constant variance.  
o The residuals are normally distributed.  

```{r fig.height = 7, fig.width = 7, fig.align = "center"}
ts.diag(model1)
```

From the first and second plots of ACF & PACF for residuals, we can see that none of the correlations are significant at all the given lags, except at lag-7 which is slightly significant. Hence, we can assume that the residuals are uncorrelated and have constant variance.  
From the third plot, which is the plot of p-values for Ljung-box statistic, we can see that the p-values at individual lags are greater that 0.05 and way above the bottom line. Therefore, we can say that the residuals are independent. Also, the residuals are found to be normally distributed from the fourth plot.  

**Box-Ljung test**  

The Box-Ljung test is used to check the lack of fit for a time series model. The test is applied to the residuals an ARIMA(p,d,q) model, which takes ‘k’ autocorrelations of the residuals. If they are very small, we can say that the model doesn’t exhibit significant lack of fit. We have tested on 18 lags.  

```{r}
Box.test(model1$residuals, lag=18)
```
Hypothesis:  
Ho: Model fits the data well/doesn’t exhibit lack of fit  
Ha: Model doesn’t fit the data well/exhibits lack of fit  
Interpretation: As p = 0.6526 > 0.05, we fail to reject Null Hypothesis. Therefore, we do not have sufficient evidence to conclude that ARIMA (5,1,0) model exhibits lack of fit.  

## Model 2: ARIMA(5,1,0) with significant lags  

Regression analysis on lags
```{r}
lag1_data = lag(ts(data_train),1) 
y = ts(data_train) - lag1_data
ylag1 = lag(y,1)
ylag2 = lag(y,2)
ylag3 = lag(y,3)
ylag4 = lag(y,4)
ylag5 = lag(y,5)

y_new = cbind(y, ylag1, ylag2, ylag3, ylag4, ylag5)
regfit = lm(y_new[,1]~y_new[,2]+y_new[,3]+y_new[,4]+y_new[,5]+y_new[,6])
summary(regfit)
```
Only lag-1 and lag-5 are signicant. Therefore fitting the below model with significant lags 1 and lag-5  

```{r}
model2 = arima(ts(data_train), order = c(5,1,0), fixed = c(NA,0,0,0,NA))
model2
```

**Residuals Diagnosis:** 
```{r}
ts.diag(model2)
```
From the first and second plots of ACF & PACF for residuals, we can see that none of the correlations are significant at all the given lags, except at lag-7 which is slightly significant. Hence, we can assume that the residuals are uncorrelated and have constant variance.  

From the third plot, which is the plot of p-values for Ljung-box statistic, we can see that the p-values at individual lags are greater than 0.05 and way above the bottom line. Therefore, we can say that the residuals are independent.  

**Box-Ljung test**
```{r}
Box.test(model2$residuals, lag=18)
```
Hypothesis:  
Ho: Model fits the data well/doesn’t exhibit lack of fit  
Ha: Model doesn’t fit the data well/exhibits lack of fit  
Interpretation: As p = 0.5746 > 0.05, we fail to reject Null Hypothesis. Therefore, we do not have sufficient evidence to conclude that the model exhibits lack of fit.  

## Model 3: Multiplicative Seasonal Model - ARIMA(5,1,0)(0,0,1)_{12}

```{r}
model3 = arima(data_train,order=c(5,1,0), seasonal=list(order=c(0,0,1),period=12))
model3
```

**Residuals Diagnosis:** 
```{r}
ts.diag(model3)
```

From the first and second plots of ACF & PACF for residuals, we can see that none of the correlations are significant at all the given lags. Hence, we can assume that the residuals are uncorrelated and have constant variance.  

From the third plot, which is the plot of p-values for Ljung-box statistic, we can see that the p-values at individual lags are greater than 0.05 and way above the bottom line. Therefore, we can say that the residuals are independent. Also, the residuals are found to be normally distributed from the fourth plot.  

**Box-Ljung test**
```{r}
Box.test(model2$residuals, lag=18)
```

Hypothesis:  
Ho: Model fits the data well/doesn’t exhibit lack of fit  
Ha: Model doesn’t fit the data well/exhibits lack of fit  

Interpretation: As p = 0.5645 > 0.05, we fail to reject Null Hypothesis. Therefore, we do not have sufficient evidence to conclude that the model exhibits lack of fit  

# Step 6: Comparison of models - AIC criterion  

```{r}
model1_AIC <- AIC(model1)
model2_AIC <- AIC(model2)
model3_AIC <- AIC(model3)
rbind(model1_AIC,model2_AIC,model3_AIC)
```

The second model i.e., ARIMA (5,1,0) with significant coefficients has the least AIC, which can be taken as the best model.  

# Step 7: Final model  

```{r}
model2
```
$$X_{t} = (0.2741 * X_{t-1}) - (0.194 * X_{t-5}) + e$$
As per the model, the average gas price of a month depends on 27.5% of the previous month and 19.4% of the past 5th month plus the unexplained market fluctuations.
Considering this as the best model, we have predicted using this model for the year 2017 for 12 months, which is given in the next section.  

# Step 8: Predictions for the year, 2017  

```{r}
predicted <- predict(model2, n.ahead = 12)
predicted_ts <- ts(predicted$pred, frequency = 12, start = 2017)
ts.plot(data_ts ,predicted_ts ,ts((predicted$pred+1.64*predicted$se),frequency = 12, start = 2017),ts((predicted$pred-1.64*predicted$se),frequency = 12,start=2017), lty = c(3,1,1,1), ylab = "Price in $", xlab = "Years")
```
```{r}
actual_test <- data_pr$Price[157:nrow(data_pr)]
(sum(abs((actual_test-predicted$pred)/actual_test))/12)*100
```
The predicted values are represented by solid line with 90% confidence intervals. The actual values are represented by a dotted line. It seems like the model fits pretty well and the mean absolute percentage error comparing with the actual and predicted values for the year 2017 is found to 4.66%.  
